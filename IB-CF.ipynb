{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4471234,"sourceType":"datasetVersion","datasetId":1031}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-16T13:02:30.979980Z","iopub.execute_input":"2024-06-16T13:02:30.980403Z","iopub.status.idle":"2024-06-16T13:02:32.237909Z","shell.execute_reply.started":"2024-06-16T13:02:30.980368Z","shell.execute_reply":"2024-06-16T13:02:32.236441Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/ecommerce-dataset/item_properties_part1.csv\n/kaggle/input/ecommerce-dataset/category_tree.csv\n/kaggle/input/ecommerce-dataset/item_properties_part2.csv\n/kaggle/input/ecommerce-dataset/events.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:32.240294Z","iopub.execute_input":"2024-06-16T13:02:32.240913Z","iopub.status.idle":"2024-06-16T13:02:33.881680Z","shell.execute_reply.started":"2024-06-16T13:02:32.240871Z","shell.execute_reply":"2024-06-16T13:02:33.880480Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Read data\nevents = pd.read_csv('/kaggle/input/ecommerce-dataset/events.csv')\n\n# Data preprocessing\nevents = events.dropna(subset=['event'])\nevents['event'] = events['event'].replace({'view': 1.0, 'addtocart': 2.0, 'transaction': 3.0})\nevents = events.drop(columns=['timestamp', 'transactionid'])","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:33.883281Z","iopub.execute_input":"2024-06-16T13:02:33.883717Z","iopub.status.idle":"2024-06-16T13:02:38.986096Z","shell.execute_reply.started":"2024-06-16T13:02:33.883673Z","shell.execute_reply":"2024-06-16T13:02:38.984767Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_33/2446743032.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  events['event'] = events['event'].replace({'view': 1.0, 'addtocart': 2.0, 'transaction': 3.0})\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data sorting / thinning\nagg_events = events.groupby('itemid').agg(mean_event=('event', 'mean'), number_of_events=('event', 'count')).reset_index()\nagg_events_GT50 = agg_events[agg_events['number_of_events'] > 50]\ndf_GT50 = pd.merge(events, agg_events_GT50[['itemid']], on='itemid', how='inner')\n\n# Sampling data for better weighting of view, addtocart events\nview_events = df_GT50[df_GT50['event'] == 1.0].sample(frac=0.01, random_state=1)\naddtocart_events = df_GT50[df_GT50['event'] == 2.0].sample(frac=0.1, random_state=1)\ntransaction_events = df_GT50[df_GT50['event'] == 3.0]\n\n# Combining sampled data\ndf_sampled = pd.concat([view_events, addtocart_events, transaction_events])\n\n# Reducing to users with high interactions\nuser_interaction_counts = df_sampled['visitorid'].value_counts()\nhigh_interaction_users = user_interaction_counts[user_interaction_counts > 10].index\ndf_reduced = df_sampled[df_sampled['visitorid'].isin(high_interaction_users)]\n\n# Implementation\nmatrix = df_reduced.pivot_table(index='visitorid', columns='itemid', values='event')\nmatrix_std = pd.DataFrame(StandardScaler().fit_transform(matrix.fillna(0)), index=matrix.index, columns=matrix.columns)\n\n# Generating similarity matrices\nitem_similarity_pearson = matrix_std.corr()\nitem_similarity_cosine = cosine_similarity(matrix_std.fillna(0).T)\nitem_similarity_cosine_df = pd.DataFrame(item_similarity_cosine, index=matrix_std.columns, columns=matrix_std.columns)\nitem_similarity_euclidean = pdist(matrix_std.fillna(0).T, 'euclidean')\n# Convert Euclidean distances to similarities\nitem_similarity_euclidean_df = pd.DataFrame(1 / (1 + squareform(item_similarity_euclidean)), index=matrix_std.columns, columns=matrix_std.columns)\n\n# Hybrid similarity matrix\nitem_similarity_hybrid = (0.5 * item_similarity_pearson + \n                          0.3 * item_similarity_cosine_df + \n                          0.2 * item_similarity_euclidean_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:38.989680Z","iopub.execute_input":"2024-06-16T13:02:38.990464Z","iopub.status.idle":"2024-06-16T13:02:42.408718Z","shell.execute_reply.started":"2024-06-16T13:02:38.990421Z","shell.execute_reply":"2024-06-16T13:02:42.407570Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Function to get top-N recommendations\ndef get_top_n_recommendations(item_ids, similarity_df, n=15, weights=None):\n    similar_items = pd.Series(dtype=float)\n    for item in item_ids:\n        if item in similarity_df:\n            similar_items = pd.concat([similar_items, similarity_df[item]])\n    similar_items = similar_items.groupby(similar_items.index).mean()\n    similar_items = similar_items.drop(item_ids, errors='ignore')\n    similar_items = similar_items.sort_values(ascending=False)\n    if weights is not None:\n        similar_items *= weights\n    return similar_items.head(n)\n\n# Leave-One-Out Cross Validation (LOOCV) function\ndef loocv_recommendation(user_viewed_items, similarity_df, n=15, weights=None):\n    if not user_viewed_items:\n        print(\"No viewed items for LOOCV.\")\n        return 0\n    hit_count = 0\n    for i in range(len(user_viewed_items)):\n        test_item = user_viewed_items[i]\n        train_items = user_viewed_items[:i] + user_viewed_items[i+1:]\n        if not train_items:\n            continue\n        recommendations = get_top_n_recommendations(train_items, similarity_df, n, weights)\n        if test_item in recommendations.index:\n            hit_count += 1\n    success_rate = hit_count / len(user_viewed_items) if user_viewed_items else 0\n    print(f\"LOOCV Success Rate: {success_rate}\")\n    return success_rate","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:42.410397Z","iopub.execute_input":"2024-06-16T13:02:42.411129Z","iopub.status.idle":"2024-06-16T13:02:42.423784Z","shell.execute_reply.started":"2024-06-16T13:02:42.411084Z","shell.execute_reply":"2024-06-16T13:02:42.422484Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Example: List of items viewed or purchased by a user\nuser_viewed_items = df_reduced[df_reduced['visitorid'] == df_reduced['visitorid'].iloc[0]]['itemid'].tolist()\nprint(f\"User viewed items: {user_viewed_items}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:42.425618Z","iopub.execute_input":"2024-06-16T13:02:42.426674Z","iopub.status.idle":"2024-06-16T13:02:42.445440Z","shell.execute_reply.started":"2024-06-16T13:02:42.426628Z","shell.execute_reply":"2024-06-16T13:02:42.443925Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"User viewed items: [392074, 134191, 314952, 338395, 280888, 361656, 46156, 85334, 119736, 458637, 186702, 371390, 353111, 119736, 422561, 359336, 448453, 86824, 403969, 287449, 280888, 141090, 321850, 314952, 72640, 338395]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate success rate for each similarity metric\nresults_cf = {}\nfor name, similarity_df in [('Pearson', item_similarity_pearson), \n                            ('Cosine', item_similarity_cosine_df), \n                            ('Euclidean', item_similarity_euclidean_df)]:\n    print(f\"Calculating success rate for {name} similarity...\")\n    success_rate = loocv_recommendation(user_viewed_items, similarity_df, n=15)\n    results_cf[name] = success_rate","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:42.447290Z","iopub.execute_input":"2024-06-16T13:02:42.447728Z","iopub.status.idle":"2024-06-16T13:02:43.188909Z","shell.execute_reply.started":"2024-06-16T13:02:42.447688Z","shell.execute_reply":"2024-06-16T13:02:43.187779Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Calculating success rate for Pearson similarity...\nLOOCV Success Rate: 0.6923076923076923\nCalculating success rate for Cosine similarity...\nLOOCV Success Rate: 0.6923076923076923\nCalculating success rate for Euclidean similarity...\nLOOCV Success Rate: 0.6923076923076923\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate success rate using hybrid similarity\nprint(\"Calculating success rate for Hybrid similarity...\")\nhybrid_success_rate = loocv_recommendation(user_viewed_items, item_similarity_hybrid, n=15)\n\n# Output results\nfor name, success_rate in results_cf.items():\n    print(f\"\\n{name} Similarity Success Rate: {success_rate}\")\nprint(f\"\\nHybrid Similarity Success Rate: {hybrid_success_rate}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-16T13:02:43.190395Z","iopub.execute_input":"2024-06-16T13:02:43.190750Z","iopub.status.idle":"2024-06-16T13:02:43.436380Z","shell.execute_reply.started":"2024-06-16T13:02:43.190721Z","shell.execute_reply":"2024-06-16T13:02:43.435084Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Calculating success rate for Hybrid similarity...\nLOOCV Success Rate: 0.6923076923076923\n\nPearson Similarity Success Rate: 0.6923076923076923\n\nCosine Similarity Success Rate: 0.6923076923076923\n\nEuclidean Similarity Success Rate: 0.6923076923076923\n\nHybrid Similarity Success Rate: 0.6923076923076923\n","output_type":"stream"}]}]}